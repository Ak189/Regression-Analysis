Linear Regression model:
Y = β_0 + β_1X + ε
(It can be extended) Here Y is the Response Variable or dependent variable while X is 
dependent variable or the predictor variable.
Response variable = Model function + Random error.
A model is call Non linear if the order is more than 1 (in X).
Y = β_0 + β_1(X) + β_2(X)^2 + ε (example of a non linear model)
ŷ (y hat) represents the predicted value of Y for given X, when β_0 and β_1 are determined.
To find the least squares just take the derivative of sum of square of function with respect 
to each coef.
The line fitted by the least squares is the one that makes the sum of squares of all these
vertical discrepancies as small as possible.
ΣX_i^2 = uncorrected sum of squares of X
ΣX_i^2/n = correction of the mean of the X's
The difference is called the sum of squares of X.
ΣX_iY_i = uncorrected sum of products.
ΣX_iY_i/n = correction of mean.
The difference is called the sum of products of X and Y.
S_xy = Σ(X_i)(Y_i) - nx̄ȳ     (covariance)
S_xx = Σ(X_i)^2 - n(X̄)^2  (corrected sum of squares)
S_yy = Σ(Y_i)^2 - n(ȳ)^2   (variability in Y)
Analysis of Variance
y_i - ŷ = y_i - ȳ - (ŷ - ȳ) (Trick to solve the anova)
We also know (Σŷ_i)/n = ȳ
Σ(ŷ_i - ȳ)^2 = (S_xy)^2/(S_xx)
Sum of squares about the mean = sum of squares due to regression + sum of squares about regression.
Sum of squares due to regression = Σ(ŷ_i - ȳ)^2 (i = 1...n), dof = 1
Sum of squares about the regression (residuals) = Σ(y_i - ŷ)^2, dof = n-2
Total sum of squares = Σ(y_i - ȳ)^2  , dof  = n-1
Degree of freedom: Sum of squares have a number associated with it, which indicates how many independent pieces of information involving the n independent numbers Y_1, Y_2, ..,Y_n are needed to compile the sum of squares.
A skeleton ANOVA consists of only the source and the dof.
R^2 = (SS due to regression)/ (TSS)
It measures the proportion of total variation about mean Y_bar explained by the regression.
It is also the square of the multiple correlation coefficient
